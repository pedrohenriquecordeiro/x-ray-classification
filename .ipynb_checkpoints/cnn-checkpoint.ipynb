{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "### constantes do problema\n",
    "\n",
    "## 2 => ( 0 - pulmao normal | 1 - pulmao com pneumonia)\n",
    "numero_classes = 2\n",
    "\n",
    "## dimensao da imagem\n",
    "# corta um quadrado na imagem\n",
    "tamanho = 120\n",
    "\n",
    "# tamanho do batch\n",
    "batch = 50\n",
    "\n",
    "# numero de epocas\n",
    "epoch = 10\n",
    "\n",
    "\n",
    "## caminhos das pastas das imagens \n",
    "\n",
    "caminho_test_normal     = \"/home/pedro/Imagens/processado_chest-xray-pneumonia/test/NORMAL/*.png\"\n",
    "caminho_test_pneumonia  = \"/home/pedro/Imagens/processado_chest-xray-pneumonia/test/PNEUMONIA/*.png\"\n",
    "\n",
    "caminho_train_normal    = \"/home/pedro/Imagens/processado_chest-xray-pneumonia/train/NORMAL/*.png\" \n",
    "caminho_train_pneumonia = \"/home/pedro/Imagens/processado_chest-xray-pneumonia/train/PNEUMONIA/*.png\"\n",
    "\n",
    "caminho_val_normal      = \"/home/pedro/Imagens/processado_chest-xray-pneumonia/val/NORMAL/*.png\"\n",
    "caminho_val_pneumonia   = \"/home/pedro/Imagens/processado_chest-xray-pneumonia/val/PNEUMONIA/*.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named keras",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2be30809342c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named keras"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import cv2 \n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### funcao para carregar a imagem do caminho\n",
    "\n",
    "def ler_imagem(caminho):\n",
    "    img = cv2.imread(caminho)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    imres = image.img_to_array(gray.T)\n",
    "    imres = np.expand_dims(imres, axis = 0)    \n",
    "    return imres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "### carrega o dataset na a memoria\n",
    "## busca o caminho de cada imagem dentro da pasta do dataset\n",
    "\n",
    "import glob\n",
    "\n",
    "arquivo_test_normal    = glob.glob(caminho_test_normal)\n",
    "arquivo_test_pneumonia = glob.glob(caminho_test_pneumonia)\n",
    "\n",
    "arquivo_train_normal    = glob.glob(caminho_train_normal)\n",
    "arquivo_train_pneumonia = glob.glob(caminho_train_pneumonia)\n",
    "\n",
    "arquivo_val_normal    = glob.glob(caminho_val_normal)\n",
    "arquivo_val_pneumonia = glob.glob(caminho_val_pneumonia)\n",
    "\n",
    "\n",
    "## busca cada imagem e salva em arrays\n",
    "\n",
    "# dados de teste normais\n",
    "test_normal = []\n",
    "for caminho_imagem in arquivo_test_normal:\n",
    "    test_normal.append(ler_imagem(caminho_imagem))\n",
    "    \n",
    "# dados de teste com pneumonia\n",
    "test_pneumonia =  []\n",
    "for caminho_imagem in arquivo_test_pneumonia:\n",
    "    test_pneumonia.append(ler_imagem(caminho_imagem))\n",
    "    \n",
    "# dados de treino normais\n",
    "train_normal =  []\n",
    "for caminho_imagem in arquivo_train_normal:\n",
    "    train_normal.append(ler_imagem(caminho_imagem))\n",
    "    \n",
    "# dados de teste pneumonia\n",
    "train_pneumonia =  []\n",
    "for caminho_imagem in arquivo_train_pneumonia:\n",
    "    train_pneumonia.append(ler_imagem(caminho_imagem))\n",
    "\n",
    "# dados de validacao normais\n",
    "val_normal =  []\n",
    "for caminho_imagem in arquivo_val_normal:\n",
    "    val_normal.append(ler_imagem(caminho_imagem))\n",
    "    \n",
    "# dados de validacao pneumonia\n",
    "val_pneumonia =  []\n",
    "for caminho_imagem in arquivo_val_pneumonia:\n",
    "    val_pneumonia.append(ler_imagem(caminho_imagem))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cria o Y do dataset\n",
    "## 0 -> pulmao normal    1 -> pulmao com pneumonia\n",
    "\n",
    "y_train = np.concatenate( \n",
    "    (\n",
    "        np.array( [0] * len(arquivo_train_normal)    ) , \n",
    "        np.array( [1] * len(arquivo_train_pneumonia) ) \n",
    "    ),\n",
    "    axis = 0 )\n",
    "\n",
    "\n",
    "y_test = np.concatenate( \n",
    "    (\n",
    "        np.array( [0] * len(arquivo_test_normal)    ) , \n",
    "        np.array( [1] * len(arquivo_test_pneumonia) ) \n",
    "    ),\n",
    "    axis = 0 )\n",
    "\n",
    "y_val = np.concatenate( \n",
    "    (\n",
    "        np.array( [0] * len(arquivo_val_normal)    ) , \n",
    "        np.array( [1] * len(arquivo_val_pneumonia) ) \n",
    "    ),\n",
    "    axis = 0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "### concatena as imagens normais com as imagens de pneumonia\n",
    "\n",
    "x_test = np.concatenate( \n",
    "    (\n",
    "        test_normal, \n",
    "        test_pneumonia \n",
    "    ),\n",
    "    axis = 0\n",
    ")\n",
    "\n",
    "\n",
    "x_train = np.concatenate( \n",
    "    (\n",
    "        train_normal, \n",
    "        train_pneumonia \n",
    "    ),\n",
    "    axis = 0 )\n",
    "    \n",
    "x_val = np.concatenate( \n",
    "    (\n",
    "        val_normal, \n",
    "        val_pneumonia \n",
    "    ),\n",
    "    axis = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "### musturar os dados X e Y\n",
    "\n",
    "import random\n",
    "\n",
    "zip_1 = list(zip(x_test, y_test))\n",
    "random.shuffle(zip_1)\n",
    "x_test, y_test = zip(*zip_1)\n",
    "\n",
    "zip_2 = list(zip(x_train, y_train))\n",
    "random.shuffle(zip_2)\n",
    "x_train, y_train = zip(*zip_2)\n",
    "\n",
    "zip_3 = list(zip(x_val, y_val))\n",
    "random.shuffle(zip_3)\n",
    "x_val, y_val = zip(*zip_3)\n",
    "\n",
    "# converte os lists para ndarray\n",
    "x_test  = np.array(x_test)\n",
    "y_test  = np.array(y_test)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_val   = np.array(x_val)\n",
    "y_val   = np.array(y_val)\n",
    "\n",
    "# rearranja os dados\n",
    "x_train = x_train.reshape(len(x_train),120,120,1)\n",
    "#y_train = y_train.reshape(len(y_train),120,120,1)\n",
    "x_test  = x_test.reshape(len(x_test),120,120,1)\n",
    "#y_test  = y_test.reshape(len(y_test),120,120,1)\n",
    "x_val   = x_val.reshape(len(x_val),120,120,1)\n",
    "#y_val   = y_val.reshape(len(y_val),120,120,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "### categoriza os Y\n",
    "\n",
    "y_test  = keras.utils.to_categorical(y_test , numero_classes)\n",
    "y_train = keras.utils.to_categorical(y_train, numero_classes)\n",
    "y_val   = keras.utils.to_categorical(y_val  , numero_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "### cria o modelo da CNN\n",
    "## referencia : https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n",
    "\n",
    "modelo = Sequential()\n",
    "\n",
    "# primeira camada convolucional + RELU\n",
    "modelo.add(\n",
    "    Conv2D(\n",
    "        filters              = 32, \n",
    "        kernel_size          = (1 , 1), \n",
    "        strides              = (1 , 1), \n",
    "        padding              = 'valid', \n",
    "        data_format          = None, \n",
    "        dilation_rate        = (1, 1), \n",
    "        activation           = 'relu', \n",
    "        use_bias             = True, \n",
    "        kernel_initializer   = 'glorot_uniform', \n",
    "        bias_initializer     = 'zeros', \n",
    "        kernel_regularizer   = l2(0.0005), \n",
    "        bias_regularizer     = None, \n",
    "        activity_regularizer = None, \n",
    "        kernel_constraint    = None, \n",
    "        bias_constraint      = None,\n",
    "        input_shape          =(tamanho,tamanho,1)\n",
    "    )\n",
    ")\n",
    "\n",
    "# segunda camada convolucional + RELU\n",
    "modelo.add(\n",
    "    Conv2D(\n",
    "        filters              = 64, \n",
    "        kernel_size          = (3 , 3), \n",
    "        strides              = (1 , 1), \n",
    "        padding              = 'valid', \n",
    "        dilation_rate        = (1, 1), \n",
    "        activation           = 'relu', \n",
    "        use_bias             = True, \n",
    "        kernel_initializer   = 'glorot_uniform', \n",
    "        bias_initializer     = 'zeros', \n",
    "        kernel_regularizer   = l2(0.0005)\n",
    "    )\n",
    ")\n",
    "\n",
    "# adiciona um polling maximo\n",
    "modelo.add(\n",
    "    MaxPooling2D(pool_size=(2, 2))\n",
    ")\n",
    "\n",
    "\n",
    "# terceira camada convolucional + RELU\n",
    "modelo.add(\n",
    "    Conv2D(\n",
    "        filters              = 128, \n",
    "        kernel_size          = (1 , 1), \n",
    "        strides              = (1 , 1), \n",
    "        padding              = 'valid', \n",
    "        dilation_rate        = (1, 1), \n",
    "        activation           = 'relu', \n",
    "        use_bias             = True, \n",
    "        kernel_initializer   = 'glorot_uniform', \n",
    "        bias_initializer     = 'zeros', \n",
    "        kernel_regularizer   = l2(0.0005)\n",
    "    )\n",
    ")\n",
    "\n",
    "# quarta camada convolucional + RELU\n",
    "modelo.add(\n",
    "    Conv2D(\n",
    "        filters              = 128, \n",
    "        kernel_size          = (3 , 3), \n",
    "        strides              = (1 , 1), \n",
    "        padding              = 'valid', \n",
    "        dilation_rate        = (1, 1), \n",
    "        activation           = 'relu', \n",
    "        use_bias             = True,\n",
    "        kernel_initializer   = 'glorot_uniform', \n",
    "        bias_initializer     = 'zeros', \n",
    "        kernel_regularizer   = l2(0.0005)\n",
    "    )\n",
    ")\n",
    "\n",
    "# adiciona um polling maximo\n",
    "modelo.add(\n",
    "    MaxPooling2D(pool_size=(2, 2))\n",
    ")\n",
    "\n",
    "'''\n",
    "TESTANDO 4 CAMADA CONVOLUCIONAIS\n",
    "# quinta camada convolucional + RELU\n",
    "modelo.add(\n",
    "    Conv2D(\n",
    "        filters              = 128, \n",
    "        kernel_size          = (1 , 1), \n",
    "        strides              = (1 , 1), \n",
    "        padding              = 'valid', \n",
    "        data_format          = None, \n",
    "        dilation_rate        = (1, 1), \n",
    "        activation           = 'relu', \n",
    "        use_bias             = True, \n",
    "        kernel_initializer   = 'glorot_uniform', \n",
    "        bias_initializer     = 'zeros', \n",
    "        kernel_regularizer   = l2(0.0005)\n",
    "    )\n",
    ")\n",
    "\n",
    "# sexta camada convolucional + RELU\n",
    "modelo.add(\n",
    "    Conv2D(\n",
    "        filters              = 128, \n",
    "        kernel_size          = (3 , 3), \n",
    "        strides              = (1 , 1), \n",
    "        padding              = 'valid', \n",
    "        dilation_rate        = (1, 1), \n",
    "        activation           = 'relu', \n",
    "        use_bias             = True, \n",
    "        kernel_initializer   = 'glorot_uniform', \n",
    "        bias_initializer     = 'zeros', \n",
    "        kernel_regularizer   = l2(0.0005)\n",
    "    )\n",
    ")\n",
    "\n",
    "# adiciona um polling maximo\n",
    "modelo.add(\n",
    "    MaxPooling2D(pool_size=(2, 2))\n",
    ")\n",
    "\n",
    "'''\n",
    "\n",
    "# desempacotar os tensores\n",
    "# sempre deve ser usada antes de entrar na \n",
    "# camada totalment conectada\n",
    "modelo.add(Flatten(\n",
    "    data_format = None\n",
    "    )\n",
    ")\n",
    "\n",
    "# cria primeira camada de NN totalmente conectada\n",
    "modelo.add(Dense(\n",
    "        units                = 128, \n",
    "        activation           = 'relu', \n",
    "        use_bias             = True, \n",
    "        kernel_initializer   = 'glorot_uniform', \n",
    "        bias_initializer     = 'zeros', \n",
    "        kernel_regularizer   = l2(0.0005)\n",
    "    )\n",
    ")\n",
    "\n",
    "#  o dropout desliga aleatoriamente alguns neurônios na rede, o que força \n",
    "# os dados a encontrar novos caminhos ... reduzindo o overfitting\n",
    "modelo.add(\n",
    "    Dropout(\n",
    "        rate        = 0.2, \n",
    "        noise_shape = None, \n",
    "        seed        = None\n",
    "    )\n",
    ")\n",
    "\n",
    "# cria segunda camada de NN totalmente conectada\n",
    "modelo.add(Dense(\n",
    "        units                = 64, \n",
    "        activation           = 'relu', \n",
    "        use_bias             = True, \n",
    "        kernel_initializer   = 'glorot_uniform', \n",
    "        bias_initializer     = 'zeros', \n",
    "        kernel_regularizer   = l2(0.0005)\n",
    "    )\n",
    ")\n",
    "\n",
    "modelo.add(\n",
    "    Dropout(\n",
    "        rate        = 0.3, \n",
    "        noise_shape = None, \n",
    "        seed        = None\n",
    "    )\n",
    ")\n",
    "\n",
    "# cria terceira camada de NN totalmente conectada\n",
    "modelo.add(Dense(\n",
    "        units                = 32, \n",
    "        activation           = 'relu', \n",
    "        use_bias             = True, \n",
    "        kernel_initializer   = 'glorot_uniform', \n",
    "        bias_initializer     = 'zeros', \n",
    "        kernel_regularizer   = l2(0.0005)\n",
    "    )\n",
    ")\n",
    "\n",
    "modelo.add(\n",
    "    Dropout(\n",
    "        rate        = 0.5, \n",
    "        noise_shape = None, \n",
    "        seed        = None\n",
    "    )\n",
    ")\n",
    "\n",
    "# cria camada de saida\n",
    "modelo.add(Dense(\n",
    "        units                = numero_classes, \n",
    "        activation           = 'softmax', \n",
    "        use_bias             = True, \n",
    "        kernel_initializer   = 'glorot_uniform', \n",
    "        bias_initializer     = 'zeros', \n",
    "        kernel_regularizer   = l2(0.0005)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compilacao da cnn\n",
    "\n",
    "modelo.compile(\n",
    "    optimizer          = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0), \n",
    "    loss               = \"categorical_crossentropy\", \n",
    "    metrics            = ['accuracy'], \n",
    "    loss_weights       = None, \n",
    "    sample_weight_mode = None, \n",
    "    weighted_metrics   = None, \n",
    "    target_tensors     = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cria callback para salvar o modelo a cada epoca finalizada\n",
    "\n",
    "callback_save = ModelCheckpoint('xray.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pedro/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5216 samples, validate on 16 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-6ddecf43461b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0minitial_epoch\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pedro/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/pedro/anaconda2/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pedro/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pedro/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pedro/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### treinamento da cnn\n",
    "\n",
    "modelo.fit(\n",
    "    x                = x_train, \n",
    "    y                = y_train, \n",
    "    batch_size       = batch , \n",
    "    epochs           = epoch, \n",
    "    verbose          = 1, \n",
    "    callbacks        = [callback_save], \n",
    "    validation_split = 0.0, \n",
    "    validation_data  = (x_val,y_val), \n",
    "    shuffle          = True, \n",
    "    class_weight     = None, \n",
    "    sample_weight    = None, \n",
    "    initial_epoch    = 0, \n",
    "    steps_per_epoch  = None, \n",
    "    validation_steps = None\n",
    ")\n",
    "\n",
    "## mostra resultado\n",
    "score = modelo.evaluate(x_test, y_test, verbose = 2)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
